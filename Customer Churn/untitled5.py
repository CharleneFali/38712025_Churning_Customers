# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lfJZe6cmB4cfSdcqCDQuwfmfPmcKvhFL
"""

# Install required packages
!pip install scikeras
!pip install keras

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score
from scikeras.wrappers import KerasClassifier
from keras.models import Sequential, Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
import joblib

from google.colab import drive
drive.mount('/content/drive')

# Load dataset
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/CustomerChurn_dataset.csv')

df.info()

df.describe()

# Select relevant features
relevant_features = ['TotalCharges', 'MonthlyCharges', 'tenure', 'SeniorCitizen', 'Partner', 'Dependents', 'Contract', 'Churn']
df_relevant = df[relevant_features]

# Define features (X) and target variable (y)
X = df_relevant.drop('Churn', axis=1)
y = df_relevant['Churn']

# Visualize feature distributions
plt.figure(figsize=(15, 10))
for i, feature in enumerate(relevant_features[:-1], 1):
    plt.subplot(3, 3, i)
    sns.countplot(x=feature, hue='Churn', data=df_relevant)
    plt.title(f'{feature} vs Churn')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Encode categorical variables
label_encoder = LabelEncoder()

for column in df_relevant.select_dtypes(include=['object']).columns:
    df_relevant[column] = label_encoder.fit_transform(df_relevant[column])

# Display the encoded dataset
df_relevant.head()

# Select numeric features and drop 'Churn'
numeric_features = ['TotalCharges', 'MonthlyCharges', 'tenure', 'SeniorCitizen']
X_numeric = df_relevant[numeric_features]

# Scale numeric features
scaled = StandardScaler()
X_scaled = scaled.fit_transform(X_numeric)

# Convert the scaled array to a DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=numeric_features)

# Select features and drop 'Churn'
selected_features = ['TotalCharges', 'MonthlyCharges', 'tenure', 'SeniorCitizen', 'Partner', 'Dependents', 'Contract']
X_selected = pd.concat([X_scaled_df, df_relevant[selected_features]], axis=1)

# Display the scaled and selected dataset
X_selected.head()

# Scale selected features
X_selected_scaled = scaled.fit_transform(X_selected)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected_scaled, y, test_size=0.2, random_state=42)

from keras.models import Model
from keras.layers import Input, Dense
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from scikeras.wrappers import KerasClassifier

def create_mlp_model(input_shape, optimizer='adam'):
    inputs = Input(shape=(input_shape,))
    hidden_layer1 = Dense(64, activation='relu')(inputs)
    hidden_layer2 = Dense(32, activation='relu')(hidden_layer1)
    hidden_layer3 = Dense(16, activation='relu')(hidden_layer2)
    output_layer = Dense(1, activation='sigmoid')(hidden_layer3)

    model = Model(inputs=inputs, outputs=output_layer)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Initialize the MLP model with verbose=1 to see epoch results
mlp_classifier = KerasClassifier(build_fn=create_mlp_model, input_shape=X_train.shape[1], verbose=1)

# Initialize GridSearchCV with the updated parameter grid
param_grid = {
    'epochs': [10, 20],
    'batch_size': [32, 64],
    'optimizer': ['adam', 'rmsprop'],
    'validation_split': [0.1, 0.2]
}

# Initialize StratifiedKFold for cross-validation
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize GridSearchCV
grid_search_cv = GridSearchCV(estimator=mlp_classifier, param_grid=param_grid, cv=stratified_kfold, scoring='accuracy', verbose=0)

# Fit the GridSearchCV to the data
grid_search_cv.fit(X_train, y_train)

#GridSearchCV delivers the best hyperparameters identified, gets the best model, and assesses its performance on the test set. For performance evaluation, the model predictions (y_pred) are compared to the actual labels (y_test), and accuracy and ROC AUC scores are produced.



print("Best Parameters: ", grid_search_cv.best_params_)

# Use the best model from GridSearchCV
best_model = grid_search_cv.best_estimator_

# Predict on the test set
y_pred = best_model.predict(X_test)

# Evaluate the model on the test set
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])

print(f'Test Accuracy: {accuracy}')
print(f'Test AUC: {roc_auc}')

# Retrain the best model on the entire training dataset
best_model.fit(X_selected, y)

y_pred_all = best_model.predict(X_selected)

# Evaluate the retrained model
accuracy_all = accuracy_score(y, y_pred_all)
roc_auc_all = roc_auc_score(y, best_model.predict_proba(X_selected)[:, 1])

print(f'Retrained Model Accuracy: {accuracy_all}')
print(f'Retrained Model AUC: {roc_auc_all}')

# Save the retrained model
joblib.dump(best_model, '/content/drive/My Drive/Colab Notebooks/Churn_model.joblib')



